# rag_module/core/generator.py

def generate_answer(query: str, context: list, model: str = "mistral"):
    """
    Mock generation combining query + retrieved text.
    Replace later with real OpenAI or Ollama call.
    """
    combined = " ".join([c["text"] for c in context])
    return f"Answer generated by {model}: Based on context of length {len(combined)}"
